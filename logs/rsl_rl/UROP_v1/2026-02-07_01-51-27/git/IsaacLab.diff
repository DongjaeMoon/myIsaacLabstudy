--- git status ---
On branch transfer_files
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   UROP/UROP_v1/agents/rsl_rl_ppo_cfg.py
	modified:   UROP/UROP_v1/env_cfg.py
	modified:   UROP/UROP_v1/mdp/events.py
	modified:   UROP/UROP_v1/scene_objects_cfg.py
	modified:   UROP/play_rsl_rl.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	env_list.txt

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/UROP/UROP_v1/agents/rsl_rl_ppo_cfg.py b/UROP/UROP_v1/agents/rsl_rl_ppo_cfg.py
index ea8c7efda3..cad5cd0cbb 100644
--- a/UROP/UROP_v1/agents/rsl_rl_ppo_cfg.py
+++ b/UROP/UROP_v1/agents/rsl_rl_ppo_cfg.py
@@ -12,7 +12,7 @@ from isaaclab.utils import configclass
 class SteelPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     num_steps_per_env = 96
     max_iterations = 10000
-    save_interval = 50
+    save_interval = 250
     experiment_name = "UROP_v1"
     empirical_normalization = True
     policy = RslRlPpoActorCriticCfg(
@@ -25,7 +25,7 @@ class SteelPPORunnerCfg(RslRlOnPolicyRunnerCfg):
         value_loss_coef=1.0,
         use_clipped_value_loss=True,
         clip_param=0.2,
-        entropy_coef=1e-3,
+        entropy_coef=5e-3,
         num_learning_epochs=5,
         num_mini_batches=4,
         # num_mini_batches=8,
diff --git a/UROP/UROP_v1/env_cfg.py b/UROP/UROP_v1/env_cfg.py
index 9d2ff49261..c47bc722f4 100644
--- a/UROP/UROP_v1/env_cfg.py
+++ b/UROP/UROP_v1/env_cfg.py
@@ -120,14 +120,14 @@ class ActionsCfg:
             "right_hip_pitch_joint", "right_hip_roll_joint", "right_hip_yaw_joint",
             "right_knee_joint", "right_ankle_pitch_joint", "right_ankle_roll_joint",
         ],
-        scale=1.0,
+        scale=0.85,
     )
 
     # 2) 허리: 조금만
     waist = mdp.JointPositionActionCfg(
         asset_name="robot",
         joint_names=["waist_yaw_joint"],
-        scale=1.0,
+        scale=0.4,
     )
 
     # 3) 팔: policy가 제어 (shoulder 크게, elbow 중간, wrist는 roll만 있음)
@@ -137,7 +137,7 @@ class ActionsCfg:
             "left_shoulder_pitch_joint", "left_shoulder_roll_joint", "left_shoulder_yaw_joint",
             "left_elbow_joint", "left_wrist_roll_joint",
         ],
-        scale=1.5,
+        scale=1.25,
     )
 
     right_arm = mdp.JointPositionActionCfg(
@@ -146,7 +146,7 @@ class ActionsCfg:
             "right_shoulder_pitch_joint", "right_shoulder_roll_joint", "right_shoulder_yaw_joint",
             "right_elbow_joint", "right_wrist_roll_joint",
         ],
-        scale=1.5,
+        scale=1.25,
     )
 
 
@@ -183,7 +183,7 @@ class RewardsCfg:
     )
     base_vel = RewTerm(
         func=mdp.base_velocity_penalty_curriculum,
-        weight=-1.0,
+        weight=-0.5,
         params={"w_lin": 1.0, "w_ang": 0.2, "w0": 0.2, "w1": 0.05, "w2": 0.03},
     )
 
@@ -200,20 +200,20 @@ class RewardsCfg:
 
     impact = RewTerm(
         func=mdp.impact_peak_penalty_curriculum,
-        weight=-0.05,
+        weight=-0.00,
         params={
             "sensor_names": ["contact_torso", "contact_lhand", "contact_rhand"],
             "force_thr_stage1": 400.0,
             "force_thr_stage2": 300.0,
             "w0": 0.0,
-            "w1": 0.05,
-            "w2": 0.10,
+            "w1": 0.03,
+            "w2": 0.05,
         },
     )
 
     action_rate = RewTerm(
         func=mdp.action_rate_penalty_curriculum,
-        weight=-1.0,
+        weight=-0.05,
         params={"w0": 0.05, "w1": 0.02, "w2": 0.01},
     )
 
@@ -222,7 +222,7 @@ class RewardsCfg:
 class TerminationsCfg:
     time_out = DoneTerm(func=mdp.time_out, time_out=True)
     fall = DoneTerm(func=mdp.robot_fallen, params={"min_root_z": 0.55})
-    drop = DoneTerm(func=mdp.object_dropped_curriculum, params={"min_z": 0.35})
+    drop = DoneTerm(func=mdp.object_dropped_curriculum, params={"min_z": 0.55})
 
 
 @configclass
@@ -234,8 +234,8 @@ class EventCfg:
         mode="reset",
         params={
             "asset_name": "robot",
-            "stage0": {"lin_x": (-0.6, 0.6), "lin_y": (-0.4, 0.4), "yaw_rate": (-1.5, 1.5)},
-            "stage1": {"lin_x": (-0.3, 0.3), "lin_y": (-0.2, 0.2), "yaw_rate": (-0.8, 0.8)},
+            "stage0": {"lin_x": (-1.0, 0.6), "lin_y": (-0.4, 0.4), "yaw_rate": (-1.5, 1.5)},
+            "stage1": {"lin_x": (-0.2, 0.2), "lin_y": (-0.2, 0.2), "yaw_rate": (-0.8, 0.8)},
             "stage2": {"lin_x": (-0.15, 0.15), "lin_y": (-0.1, 0.1), "yaw_rate": (-0.4, 0.4)},
         },
     )
diff --git a/UROP/UROP_v1/mdp/events.py b/UROP/UROP_v1/mdp/events.py
index 6095abad85..d64d103786 100644
--- a/UROP/UROP_v1/mdp/events.py
+++ b/UROP/UROP_v1/mdp/events.py
@@ -154,6 +154,22 @@ def reset_and_toss_object_curriculum(
     vel6[:, 2] = torch.empty(n, device=device).uniform_(cfg["vel_z"][0], cfg["vel_z"][1])
     obj.write_root_velocity_to_sim(vel6, env_ids=env_ids)
 
+def randomize_object_mass(
+    env: ManagerBasedRLEnv,
+    env_ids: torch.Tensor | None,
+    asset_name: str,
+    mass_range: tuple[float, float],
+):
+    asset: RigidObject = env.scene[asset_name]
+    if env_ids is None:
+        env_ids = env.scene.env_indices
+        
+    # 랜덤 질량 생성
+    mass = torch.empty(len(env_ids), device=env.device).uniform_(*mass_range)
+    
+    # 질량 적용
+    asset.root_physx_view.set_masses(mass, indices=env_ids)
+
 
 # ---------------------------
 # Push recovery perturbation (interval mode)
diff --git a/UROP/UROP_v1/scene_objects_cfg.py b/UROP/UROP_v1/scene_objects_cfg.py
index c05ebb541e..7483e779d4 100644
--- a/UROP/UROP_v1/scene_objects_cfg.py
+++ b/UROP/UROP_v1/scene_objects_cfg.py
@@ -16,8 +16,8 @@ dj_robot_cfg = ArticulationCfg(
     prim_path="{ENV_REGEX_NS}/Robot",
     spawn=sim_utils.UsdFileCfg(
         # ★ 동재님이 만드신 최종 파일 경로
-        #usd_path="/home/roro_common/mdj/IsaacLab/UROP/UROP_v0/usd/G1_23DOF_UROP.usd",
-        usd_path="/home/dongjae/isaaclab/myIsaacLabstudy/UROP/UROP_v1/usd/G1_23DOF_UROP.usd",
+        usd_path="/home/roro_common/mdj/IsaacLab/UROP/UROP_v0/usd/G1_23DOF_UROP.usd",
+        #usd_path="/home/dongjae/isaaclab/myIsaacLabstudy/UROP/UROP_v1/usd/G1_23DOF_UROP.usd",
         activate_contact_sensors=True, # 충격 감지를 위해 필수
     ),
     init_state=ArticulationCfg.InitialStateCfg(
@@ -54,9 +54,9 @@ bulky_object_cfg = RigidObjectCfg(
         visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 0.8, 0.0)),
         # [추가됨] 마찰력 설정: 고무나 거친 종이 박스처럼 마찰을 높임
         physics_material=RigidBodyMaterialCfg(
-            static_friction=1.0,  # 정지 마찰계수 (높게)
-            dynamic_friction=1.0, # 운동 마찰계수 (높게)
-            restitution=0.1,      # 반발계수 (튕겨나가지 않게 0.1으로)
+            static_friction=2.0,  # 정지 마찰계수 (높게)
+            dynamic_friction=2.0, # 운동 마찰계수 (높게)
+            restitution=0.02,      # 반발계수 (튕겨나가지 않게 0.1으로)
         ),
         rigid_props=sim_utils.RigidBodyPropertiesCfg(
             kinematic_enabled=False,  # True로 하면 중력을 무시하고 그 자리에 고정됨 (또는 코드로 위치 제어 가능)
diff --git a/UROP/play_rsl_rl.py b/UROP/play_rsl_rl.py
index 142620271a..6c9085ce86 100644
--- a/UROP/play_rsl_rl.py
+++ b/UROP/play_rsl_rl.py
@@ -186,6 +186,8 @@ def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agen
         with torch.inference_mode():
             # agent stepping
             actions = policy(obs)
+            #play noise 
+            #actions += torch.randn_like(actions) * 0.3
             # env stepping
             obs, _, _, _ = env.step(actions)
         if args_cli.video: